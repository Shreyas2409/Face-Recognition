{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreyas2409/Face-Recognition/blob/master/ML_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvSFQmVPtqLK"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "Test data -\n",
        "https://drive.google.com/drive/folders/1ZVVk4RyGaOHm_Q87YIthSCcG_P6zS5Es?usp=drive_link\n",
        " we have made few modifications to train the model pls use this to run it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofiN7r8CcArd"
      },
      "source": [
        "Train Data - /content/drive/MyDrive/COEN240/train\n",
        "\n",
        "Test Data  - /content/drive/MyDrive/COEN240/faces (model training)\n",
        "\n",
        "Test Data - /content/drive/MyDrive/COEN240/test\n",
        "\n",
        "Test Labels - /content/drive/MyDrive/COEN240/test/labels.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S5Oj4JTiPyDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8MslZvfdOTdi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xxtm8gUzhE6"
      },
      "source": [
        "PCA With SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZgonO5A35p7",
        "outputId": "74a1218b-bb2f-41ca-d0f0-36b55fcafe08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvrl-Fat4V-m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "5f39ee8b-5d7a-4dd3-9835-14db1ed21af2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/COEN240/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-138bd63275c7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mTest2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/COEN240/test\"\u001b[0m \u001b[0;31m# Test data to evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/COEN240/test/labels.txt\"\u001b[0m \u001b[0;31m# Labels of Test data to evaluate the model after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.DS_Store'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/COEN240/train'"
          ]
        }
      ],
      "source": [
        "Train = \"/content/drive/MyDrive/COEN240/train\" # Training dataset\n",
        "Test = \"/content/drive/MyDrive/COEN240/faces\"  # Modified Test dataset used for model training\n",
        "Test2 = \"/content/drive/MyDrive/COEN240/test\" # Test data to evaluate the model\n",
        "Labels = \"/content/drive/MyDrive/COEN240/test/labels.txt\" # Labels of Test data to evaluate the model after training\n",
        "categories = os.listdir(Train)\n",
        "categories.remove('.DS_Store')\n",
        "data_train = []\n",
        "data_test = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMYmkH0_4hVl"
      },
      "outputs": [],
      "source": [
        "def crop_face(image):\n",
        "\n",
        "\n",
        "    # Zooming\n",
        "    zoom_scale = 1.4\n",
        "    image = cv2.resize(image, None, fx=zoom_scale, fy=zoom_scale, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "     # Convert the image to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Perform face detection\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "\n",
        "\n",
        "        face = gray[y:y+h, x:x+w]\n",
        "\n",
        "\n",
        "        # Apply histogram equalization\n",
        "        face = cv2.equalizeHist(face)\n",
        "\n",
        "        # Resize the face image to a fixed size\n",
        "        face = cv2.resize(face, (50, 50))\n",
        "\n",
        "        # Normalize the pixel values to [0, 1]\n",
        "        face = face.astype(np.float32) / 255.0\n",
        "\n",
        "        face = face.flatten()\n",
        "\n",
        "        return face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2f_LoLS4lON"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "for category in categories:\n",
        "        path = os.path.join(Test, category)\n",
        "        try:\n",
        "            label = categories.index(category)\n",
        "            for img in os.listdir(path):\n",
        "                imgpath = os. path.join(path, img)\n",
        "                image = cv2.imread(imgpath)\n",
        "                if image is not None:\n",
        "                    image_pre = crop_face(image)\n",
        "                    if image_pre is not None:\n",
        "                        data_test.append([image_pre,label])\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "            print(data_test)\n",
        "\n",
        "for category in categories:\n",
        "        path = os.path.join(Train, category)\n",
        "        try :\n",
        "            label = categories.index(category)\n",
        "            for img in os.listdir(path):\n",
        "                imgpath = os. path.join(path, img)\n",
        "                image = cv2.imread(imgpath)\n",
        "                if image is not None:\n",
        "                    image_pre = crop_face(image)\n",
        "                    if image_pre is not None:\n",
        "                        data_train.append([image_pre,label])\n",
        "        except Exception as e:\n",
        "            print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fyl9XxQtnqJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT4dfkIW6q_-"
      },
      "outputs": [],
      "source": [
        "\n",
        "pick_in = open('face_train.pickle', 'wb')\n",
        "pickle.dump(data_train, pick_in)\n",
        "pick_in.close()\n",
        "\n",
        "\n",
        "pick_in = open('face_test.pickle', 'wb')\n",
        "pickle.dump(data_test, pick_in)\n",
        "pick_in.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUtNGoUf7AmG"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "from sklearn.decomposition import PCA as RandomizedPCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score, recall_score\n",
        "import random\n",
        "\n",
        "pick_in = open('face_train.pickle', 'rb')\n",
        "data_train = pickle.load(pick_in)\n",
        "pick_in.close()\n",
        "\n",
        "pick_in = open('face_test.pickle', 'rb')\n",
        "data_test = pickle.load(pick_in)\n",
        "pick_in.close()\n",
        "\n",
        "\n",
        "\n",
        "categories = os.listdir(Train)\n",
        "categories.remove('.DS_Store')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkIiGc567K6i"
      },
      "outputs": [],
      "source": [
        "xtrain = []\n",
        "ytrain = []\n",
        "xtest = []\n",
        "ytest = []\n",
        "\n",
        "feature =[]\n",
        "labels = []\n",
        "\n",
        "\n",
        "for i in data_train:\n",
        "\n",
        "    xtrain.append(i[0])\n",
        "    ytrain.append(i[1])\n",
        "\n",
        "for i in data_test:\n",
        "\n",
        "    xtest.append(i[0])\n",
        "    ytest.append(i[1])\n",
        "\n",
        "\n",
        "\n",
        "# For dimensionality reduction\n",
        "\n",
        "pca = PCA(n_components=120)\n",
        "xtrain = pca.fit_transform(xtrain)\n",
        "xtest = pca.transform(xtest)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "xtrain = scaler.fit_transform(xtrain)\n",
        "xtest = scaler.transform(xtest)\n",
        "\n",
        "exp_var = sum(pca.explained_variance_ratio_ * 100)\n",
        "print('Variance explained:', exp_var)\n",
        "\n",
        "\n",
        "model = SVC(C=20, gamma=0.002,kernel='rbf',class_weight='balanced')\n",
        "model.fit(xtrain,ytrain)\n",
        "\n",
        "y_pred = model.predict(xtest)\n",
        "print('Accuracy: %.3f' % accuracy_score(ytest, y_pred))\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(ytest, y_pred,average='micro')\n",
        "print(recall)\n",
        "\n",
        "\n",
        "# #Save the model\n",
        "with open('model_svm_rbf.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Save the model\n",
        "with open('model_pca.pkl', 'wb') as f:\n",
        "    pickle.dump(pca, f)\n",
        "\n",
        "with open('model_scalar_standard.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_f-m72S7Pfa"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "# Load the saved model\n",
        "with open('model_svm_rbf.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# Load the saved model\n",
        "with open('model_pca.pkl', 'rb') as t:\n",
        "    pca = pickle.load(t)\n",
        "\n",
        "# Load the saved model\n",
        "with open('model_scalar_standard.pkl', 'rb') as t:\n",
        "    scaler = pickle.load(t)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YACtM5MV8qYv"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "def label_parse(Labelsfile):\n",
        "  labels = {}\n",
        "  with open(Labels, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip().split()\n",
        "            image_name, label = line[0], line[1]\n",
        "            labels[image_name] = label\n",
        "  return labels\n",
        "\n",
        "def copy_original_images(Test2, output_folder):\n",
        "    image_folder = os.path.join(Test2)\n",
        "    label_file_path = os.path.join(output_folder, 'label.txt')\n",
        "\n",
        "\n",
        "    labels =label_parse(Labels)\n",
        "    print(labels)\n",
        "\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "\n",
        "    for image_name, label in labels.items():\n",
        "            image_path = os.path.join(image_folder, image_name)\n",
        "            output_label_folder = os.path.join(output_folder, label)\n",
        "            os.makedirs(output_label_folder, exist_ok=True)\n",
        "            output_path = os.path.join(output_label_folder, image_name)\n",
        "\n",
        "            if os.path.exists(image_path):\n",
        "              shutil.copy(image_path, output_path)\n",
        "            else:\n",
        "                print(f\"Warning: Source file not found at {image_path}\")\n",
        "\n",
        "output_folder = \"/content/output_folder\"\n",
        "\n",
        "copy_original_images(Test2, output_folder)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPBbUsKWEnP8"
      },
      "outputs": [],
      "source": [
        "dataset = '/content/output_folder'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKOjyaIP8Ykp"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for category in categories:\n",
        "        path = os.path.join(dataset, category)\n",
        "        try :\n",
        "            label = categories.index(category)\n",
        "            for img in os.listdir(path):\n",
        "                imgpath = os. path.join(path, img)\n",
        "                image = cv2.imread(imgpath)\n",
        "                if image is not None:\n",
        "                    image_pre = crop_face(image)\n",
        "                    if image_pre is not None:\n",
        "                        data.append([image_pre,label])\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "xtest = []\n",
        "ytest = []\n",
        "for i in data:\n",
        "    xtest.append(i[0])\n",
        "    ytest.append(i[1])\n",
        "\n",
        "X_scaled_train = pca.transform(xtest)\n",
        "\n",
        "\n",
        "X_scaled_train = scaler.transform(X_scaled_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_scaled_train)\n",
        "\n",
        "print('Accuracy: %.3f' % accuracy_score(ytest, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2HkfXgUzrfy"
      },
      "source": [
        "RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTp3wcKoaZiZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pickle\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load data\n",
        "pick_in = open('face_train.pickle', 'rb')\n",
        "data_train = pickle.load(pick_in)\n",
        "pick_in.close()\n",
        "\n",
        "pick_in = open('face_test.pickle', 'rb')\n",
        "data_test = pickle.load(pick_in)\n",
        "pick_in.close()\n",
        "\n",
        "# Prepare data\n",
        "xtrain, ytrain = zip(*data_train)\n",
        "xtest, ytest = zip(*data_test)\n",
        "\n",
        "# For dimensionality reduction\n",
        "pca = PCA(n_components=120)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=52)\n",
        "\n",
        "# Create a pipeline with PCA, scaling, and Random Forest\n",
        "pipeline = Pipeline([\n",
        "    ('pca', pca),\n",
        "    ('scaler', scaler),\n",
        "    ('rf', rf_model)\n",
        "])\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {\n",
        "    'pca__n_components': [80, 100, 120],\n",
        "    'rf__n_estimators': [100, 150, 200],\n",
        "    'rf__max_depth': [None, 10, 20],\n",
        "    'rf__min_samples_split': [2, 5, 10],\n",
        "    'rf__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy')\n",
        "grid_search.fit(xtrain, ytrain)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Train Random Forest model with the best hyperparameters\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "best_pipeline.fit(xtrain, ytrain)\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred = best_pipeline.predict(xtest)\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = accuracy_score(ytest, y_pred)\n",
        "f1 = f1_score(ytest, y_pred, average='micro')\n",
        "recall = recall_score(ytest, y_pred, average='micro')\n",
        "\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Recall: {recall}')\n",
        "\n",
        "# Save the best Random Forest model\n",
        "with open('model_random_forest.pkl', 'wb') as f:\n",
        "    pickle.dump(best_pipeline, f)\n",
        "\n",
        "# Save PCA and scaler for future use\n",
        "with open('model_pca.pkl', 'wb') as f:\n",
        "    pickle.dump(pca, f)\n",
        "\n",
        "with open('model_scalar_standard.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5O_GEj4zvlL"
      },
      "source": [
        "GridSearch SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rgg7JnD8aZ3X"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma' : [0.0001, 0.001, 0.1,1],\n",
        "    'kernel' : ['rbf']\n",
        "}\n",
        "\n",
        "svc = SVC(probability=True)\n",
        "\n",
        "\n",
        "model = GridSearchCV(svc,param_grid,cv=4)\n",
        "model.fit(xtrain, ytrain)\n",
        "\n",
        "\n",
        "print(model.best_params_)\n",
        "\n",
        "y_pred = model.predict(xtest)\n",
        "print(f\"accuracy  {accuracy_score(y_pred,ytest) * 100}% \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e6BHPDgz5B7"
      },
      "source": [
        "Stochastic gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5QggWtbaioN"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(StandardScaler(), SGDClassifier(loss='log_loss'))\n",
        "\n",
        "\n",
        "model.fit(xtrain, ytrain)\n",
        "\n",
        "\n",
        "y_pred = model.predict(xtest)\n",
        "\n",
        "\n",
        "accuracy = np.mean(y_pred == ytest)\n",
        "print('Accuracy:', accuracy)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}